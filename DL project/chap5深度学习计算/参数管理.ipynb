{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e38b9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from d2l import torch as d2l\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1bae6ae",
   "metadata": {},
   "source": [
    "## 参数管理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45726592",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=nn.Sequential(\n",
    "    nn.Linear(4,10),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(10,1)\n",
    ")\n",
    "x=torch.randn(size=(2,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcfa1fdd",
   "metadata": {},
   "source": [
    "#### 访问参数\n",
    "\n",
    "二、model.state_dict()方法\n",
    "pytorch 中的 state_dict 是一个简单的python的字典对象,将每一层与它的对应参数建立映射关系.(如model的每一层的weights及偏置等等)\n",
    "\n",
    "注意：\n",
    "\n",
    "（1）只有那些参数可以训练的layer才会被保存到模型的state_dict中,如卷积层,线性层等等，像什么池化层、BN层这些本身没有参数的层是没有在这个字典中的；\n",
    "\n",
    "（2）这个方法的作用一方面是方便查看某一个层的权值和偏置数据，另一方面更多的是在模型保存的时候使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d765da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.weight \t torch.Size([10, 4])\n",
      "0.bias \t torch.Size([10])\n",
      "2.weight \t torch.Size([1, 10])\n",
      "2.bias \t torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "for params in net.state_dict():\n",
    "    print(params,'\\t',net.state_dict()[params].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c561e2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.nn.parameter.Parameter'>\n",
      "Parameter containing:\n",
      "tensor([-0.0646], requires_grad=True)\n",
      "tensor([-0.0646])\n",
      "Sequential(\n",
      "  (0): Linear(in_features=4, out_features=10, bias=True)\n",
      "  (1): ReLU()\n",
      "  (2): Linear(in_features=10, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(type(net[2].bias))\n",
    "print(net[2].bias)\n",
    "print(net[2].bias.data)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47e8fc80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.weight', torch.Size([10, 4])), ('0.bias', torch.Size([10])), ('2.weight', torch.Size([1, 10])), ('2.bias', torch.Size([1]))]\n",
      "('weight', torch.Size([10, 4])) ('bias', torch.Size([10]))\n"
     ]
    }
   ],
   "source": [
    "print([(name,params.shape) for name,params in net.named_parameters()])\n",
    "print(*[(name,params.shape) for name,params in net[0].named_parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc149cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.0646])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()[\"2.bias\"].data#state_dict()可以看作是一个字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59f7750c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0173],\n",
       "        [0.0173]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def block1():\n",
    "    return nn.Sequential(nn.Linear(4,8),nn.ReLU(),nn.Linear(8,4),nn.ReLU())\n",
    "def block2():\n",
    "    net=nn.Sequential()\n",
    "    for i in range(4):\n",
    "        net.add_module(f'block{i}',block1())#add_module 很有用\n",
    "    return net\n",
    "nett=nn.Sequential(block2(),nn.Linear(4,1))\n",
    "nett(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f774791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Sequential(\n",
      "    (block0): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block1): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block2): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "    (block3): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=8, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=8, out_features=4, bias=True)\n",
      "      (3): ReLU()\n",
      "    )\n",
      "  )\n",
      "  (1): Linear(in_features=4, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(nett)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be1456e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2152, -0.2384,  0.0516,  0.3022])\n"
     ]
    }
   ],
   "source": [
    "print(nett[0][1][2].bias.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c8a3f6",
   "metadata": {},
   "source": [
    "#### 自定义参数：正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e684704a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "[('0.weight', Parameter containing:\n",
      "tensor([[-0.0089,  0.0011, -0.0025,  0.0136],\n",
      "        [-0.0081, -0.0195, -0.0001,  0.0036],\n",
      "        [-0.0111,  0.0025,  0.0087,  0.0053],\n",
      "        [ 0.0102, -0.0029, -0.0047,  0.0051],\n",
      "        [-0.0072, -0.0208,  0.0160, -0.0070],\n",
      "        [-0.0074, -0.0034, -0.0064,  0.0139],\n",
      "        [ 0.0037,  0.0013,  0.0082, -0.0129],\n",
      "        [ 0.0076, -0.0053, -0.0078, -0.0069],\n",
      "        [ 0.0017,  0.0077,  0.0272, -0.0026],\n",
      "        [ 0.0067,  0.0062,  0.0166, -0.0063]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)), ('2.weight', Parameter containing:\n",
      "tensor([[-0.0029, -0.0006, -0.0046, -0.0095, -0.0005, -0.0069,  0.0024,  0.0008,\n",
      "          0.0014,  0.0038]], requires_grad=True)), ('2.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "def init_normal(m):#自定义参数：正态分布\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.normal_(m.weight,mean=0,std=0.01)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_normal)#pytorch中的model.apply(fn)会递归地将函数fn应用到父模块的每个子模块submodule，也包括model这个父模块自身。\n",
    "print(net[0].bias.data)\n",
    "print([(name,params) for name,params in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf704c9b",
   "metadata": {},
   "source": [
    "#### 自定义参数：常数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f4ec511e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.weight', Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)), ('2.weight', Parameter containing:\n",
      "tensor([[1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]], requires_grad=True)), ('2.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "def init_constant(m):#自定义参数：常量\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,1)#注意函数的拼写\n",
    "        nn.init.zeros_(m.bias)\n",
    "net.apply(init_constant)\n",
    "print([(name,params) for name,params in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6a543f",
   "metadata": {},
   "source": [
    "#### 注意，Xavier_uniform_是一种很好的初始化神经网络权重的方法, 对tanh很有效果，但是relu函数表现很差\n",
    "#### 作为补充，使用kaiming函数进行初始化，可以在relu函数上很好的表现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "26546ec8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.weight', Parameter containing:\n",
      "tensor([[ 0.2235, -0.2157, -0.4504, -0.6087],\n",
      "        [ 0.0236,  0.3859, -0.6317, -0.1297],\n",
      "        [ 0.1158,  0.0858, -0.5168, -0.4140],\n",
      "        [-0.1544, -0.4032, -0.3478, -0.3715],\n",
      "        [ 0.0038,  0.6455,  0.1789, -0.1064],\n",
      "        [-0.1521,  0.3185,  0.2916,  0.2536],\n",
      "        [-0.4378,  0.1692,  0.3530,  0.2215],\n",
      "        [-0.5978,  0.6303, -0.0479,  0.3094],\n",
      "        [-0.0064, -0.3371,  0.2196, -0.0501],\n",
      "        [-0.5319, -0.5221, -0.6529,  0.2456]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)), ('2.weight', Parameter containing:\n",
      "tensor([[42., 42., 42., 42., 42., 42., 42., 42., 42., 42.]],\n",
      "       requires_grad=True)), ('2.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "def Xavier(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.zeros_(m.bias)\n",
    "def init_constant(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.constant_(m.weight,42)\n",
    "        nn.init.zeros_(m.bias)\n",
    "net[0].apply(Xavier)\n",
    "net[2].apply(init_constant)\n",
    "print([(name,params) for name,params in net.named_parameters()])            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eccf9dc5",
   "metadata": {},
   "source": [
    "#### TMD 太抽象了这段\n",
    "#### 实现：参数自定义\n",
    "$$ f(x)=\\left\\{\n",
    "\\begin{aligned}\n",
    "U(5,10) 可能性\\frac{1}{4} \\\\\n",
    "0 可能性\\frac{1}{2} \\\\\n",
    "U(-10,-5) 可能性\\frac{1}{4}\\\\\n",
    "\\end{aligned}\n",
    "\\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a35f516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('0.weight', Parameter containing:\n",
      "tensor([[-5.9383, -8.9986, -0.0000,  6.7914],\n",
      "        [ 9.8173,  5.6307, -0.0000,  8.0374],\n",
      "        [ 0.0000,  7.8136,  0.0000,  6.4271],\n",
      "        [ 0.0000,  0.0000,  7.4778,  9.2649],\n",
      "        [-0.0000,  8.5363,  0.0000, -0.0000],\n",
      "        [-0.0000, -9.3629,  7.5027,  0.0000],\n",
      "        [-0.0000, -9.7523, -8.8350,  9.6947],\n",
      "        [-0.0000, -8.3388,  5.1251, -6.9979],\n",
      "        [ 0.0000, -0.0000, -5.0203, -7.7817],\n",
      "        [-0.0000, -0.0000, -9.7697,  0.0000]], requires_grad=True)), ('0.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)), ('2.weight', Parameter containing:\n",
      "tensor([[-6.0311, -9.9391, -9.8441,  8.2084, -5.8184,  5.2010,  6.8500,  0.0000,\n",
      "         -5.5040,  0.0000]], requires_grad=True)), ('2.bias', Parameter containing:\n",
      "tensor([0.], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "def init_self(m):\n",
    "    if type(m)==nn.Linear:\n",
    "        nn.init.uniform_(m.weight,-10,10)\n",
    "        m.weight.data=m.weight.data*(m.weight.data.abs()>=5)\n",
    "net.apply(init_self)\n",
    "print([(name,params) for name,params in net.named_parameters()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bce8c030",
   "metadata": {},
   "source": [
    "# 参数共享：重点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "29b1c686",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n",
      "tensor([[ 1.0000e+02, -1.8356e-01,  1.2982e-04,  2.4792e-01, -2.4462e-02,\n",
      "         -2.8970e-01, -2.2396e-01,  1.2569e-02,  2.7265e-01, -1.8294e-01],\n",
      "        [-1.7607e-01, -3.0515e-01, -2.7216e-01, -3.0695e-01,  2.4779e-01,\n",
      "          1.7051e-01, -1.2374e-01,  2.1619e-01, -2.0694e-01,  1.4515e-01],\n",
      "        [-1.5925e-01,  1.0629e-01, -2.1854e-01, -6.6662e-02, -2.1936e-01,\n",
      "          1.8106e-01, -9.8113e-02, -9.8777e-02,  1.7046e-01, -1.6635e-01],\n",
      "        [ 1.0994e-01, -2.7665e-01,  2.3554e-01,  1.7212e-01, -2.6597e-01,\n",
      "         -6.9543e-02,  6.0759e-02, -1.9855e-01, -1.9394e-01, -1.4476e-01],\n",
      "        [ 5.3052e-02,  2.3836e-01, -1.4869e-01, -2.9405e-01,  1.5892e-01,\n",
      "          4.0728e-02,  3.5410e-02, -2.0660e-01,  2.8116e-01,  7.6122e-02],\n",
      "        [ 2.4676e-01, -7.9684e-02, -5.4888e-02, -5.9521e-02,  1.7183e-01,\n",
      "         -7.1930e-02, -1.8016e-01,  2.8798e-01,  3.0286e-01,  1.9354e-01],\n",
      "        [-1.6233e-01,  1.0586e-01,  5.4799e-02, -9.9604e-02, -1.7921e-01,\n",
      "         -1.6669e-01, -1.9531e-01,  2.2385e-01,  2.1146e-01,  1.3955e-01],\n",
      "        [-1.4809e-01,  1.7887e-01,  1.0653e-01, -2.0010e-01,  1.5878e-02,\n",
      "         -4.7482e-02,  5.7707e-02,  1.1881e-01, -2.4736e-01, -1.1866e-01],\n",
      "        [-2.2372e-01,  2.9489e-01,  6.0371e-02, -1.3543e-01,  4.3853e-02,\n",
      "         -2.2790e-01,  2.7774e-01,  9.3881e-02,  1.6093e-01,  2.3420e-02],\n",
      "        [-2.3427e-01, -2.5749e-01,  2.6333e-01, -2.7429e-01, -1.9907e-01,\n",
      "         -1.9202e-01,  2.2784e-01,  5.3405e-02, -1.1096e-01,  8.0270e-02]])\n",
      "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True],\n",
      "        [True, True, True, True, True, True, True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "share=nn.Linear(10,10)#相同层实现参数共享\n",
    "net=nn.Sequential(\n",
    "    nn.Linear(4,10),\n",
    "    nn.ReLU(),\n",
    "    share,nn.ReLU(),\n",
    "    share,nn.ReLU(),\n",
    "    nn.Linear(10,1)\n",
    ")\n",
    "print(net[2].weight.data==net[4].weight.data)\n",
    "net[2].weight.data[0][0]=100\n",
    "print(net[2].weight.data)\n",
    "print(net[2].weight.data==net[4].weight.data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
